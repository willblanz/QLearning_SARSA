{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Tabular Reinforcement Learning\n",
    "## Grid World\n",
    "\n",
    "A simple  demonstration of tabular reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "saving_dpi = 100\n",
    "random_seed = 1234\n",
    "epsilon = 0.1\n",
    "num_episodes = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Grid Worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell:\n",
    "    \n",
    "    # Constructor which sets position and kind of a cell\n",
    "    def __init__(self, parent, label, position, kind = 'empty'):\n",
    "        \"\"\"Contructs a new cell object\n",
    "        Arguments\n",
    "            parent: the world to which this cell belongs\n",
    "            label: a string label for this cell\n",
    "            position: the (row, col) coordinates of this cell\n",
    "            kind: a string indicating the kind of cell\n",
    "        \"\"\"\n",
    "        self.parent = parent\n",
    "        self.label = label\n",
    "        self.position = position\n",
    "        self.kind = kind\n",
    "        self.actions = dict()\n",
    "\n",
    "    # Generate the actions available from a cell, the cells they will bring an agent to, and the reward that will be returned\n",
    "    def generate_action_state_pairs(self, rewards):\n",
    "        \n",
    "        # Add actions that are available from this cell by looking at cordiantes and find the cell that we end up in next\n",
    "        self.actions = dict()\n",
    "\n",
    "        # If the agent can go up from this cell, setup an up action\n",
    "        if self.position[0] > 0:\n",
    "            next_cell = self.parent.find(self.position[0] - 1, self.position[1])\n",
    "            reward = rewards[next_cell.kind]\n",
    "            # If the cell was a trap go back to the beginning\n",
    "            #if next_cell.kind == 'trap': \n",
    "            #    next_cell = self.parent.start_cell\n",
    "            self.actions['up'] = {'state':next_cell, 'reward':reward}\n",
    "        else:\n",
    "            reward = rewards[self.kind]\n",
    "            self.actions['up'] = {'state':self, 'reward':reward}\n",
    "            \n",
    "        # If the agent can go down from this cell, setup an down action\n",
    "        if self.position[0] < (self.parent.num_rows - 1):\n",
    "            next_cell = self.parent.find(self.position[0] + 1, self.position[1])\n",
    "            reward = rewards[next_cell.kind]\n",
    "            # If the cell was a trap go back to the beginning\n",
    "            #if next_cell.kind == 'trap': \n",
    "            #    next_cell = self.parent.start_cell\n",
    "            self.actions['down'] = {'state':next_cell, 'reward':reward}\n",
    "        else:\n",
    "            reward = rewards[self.kind]\n",
    "            self.actions['down'] = {'state':self, 'reward':reward}\n",
    "            \n",
    "        # If the agent can go left from this cell, setup an left action\n",
    "        if self.position[1] > 0:\n",
    "            next_cell = self.parent.find(self.position[0], self.position[1] - 1)\n",
    "            reward = rewards[next_cell.kind]\n",
    "            # If the cell was a trap go back to the beginning\n",
    "            #if next_cell.kind == 'trap': \n",
    "            #    next_cell = self.parent.start_cell\n",
    "            self.actions['left'] = {'state':next_cell, 'reward':reward}\n",
    "        else:\n",
    "            reward = rewards[self.kind]\n",
    "            self.actions['left'] = {'state':self, 'reward':reward}\n",
    "            \n",
    "        # If the agent can go right from this cell, setup an right action            \n",
    "        if self.position[1] < (self.parent.num_cols - 1):\n",
    "            next_cell = self.parent.find(self.position[0], self.position[1] + 1)\n",
    "            reward = rewards[next_cell.kind]\n",
    "            # If the cell was a trap go back to the beginning\n",
    "            #if next_cell.kind == 'trap': \n",
    "            #    next_cell = self.parent.start_cell \n",
    "            self.actions['right'] = {'state':next_cell, 'reward':reward}\n",
    "        else:\n",
    "            reward = rewards[self.kind]\n",
    "            self.actions['right'] = {'state':self, 'reward':reward}\n",
    "            \n",
    "    # Genereate as string representation of the cell. Wh\n",
    "    def to_string(self, details = False):\n",
    "        s = \"\"\n",
    "        if(details):\n",
    "            s = self.label + \"(\" + self.kind + \") \"\n",
    "            for  a in self.actions.keys():\n",
    "                s = s + a + \"(\" + self.actions[a]['state'].label + \"), \"\n",
    "        else:\n",
    "            s = self.label\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    \n",
    "    def __init__(self, num_rows = 10, num_cols = 10, special_cells = None):\n",
    "        \"\"\"Contructs a new world\n",
    "        Arguments\n",
    "            num_rows: the size of the world in rows\n",
    "            num_cols: the size of the world in cols\n",
    "            special_cells: a list of special cells given as ((row, col), kind) pairs for special cells. Kinds allowed are 'start', 'empty', 'trap', and 'goal'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set up rewards returned by the world\n",
    "        self.rewards = dict()\n",
    "        self.rewards['start'] = 0\n",
    "        self.rewards['empty'] = -1\n",
    "        self.rewards['trap'] = -2\n",
    "        self.rewards['trap1'] = -2\n",
    "        self.rewards['trap2'] = -10\n",
    "        self.rewards['trap3'] = -100\n",
    "        self.rewards['goal'] = 50\n",
    "        self.allowed_special_cells = ['start', 'empty', 'trap', 'trap1', 'trap2', 'trap3', 'goal']\n",
    "        \n",
    "        # Attributes to store world and world details\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.cells = list()\n",
    "        \n",
    "        # Generate a grid of cells\n",
    "        idx = 0\n",
    "        for r in range(0, num_rows):\n",
    "            for c in range(0, num_cols):\n",
    "                \n",
    "                # Generate labels that have consisten number of digits\n",
    "                l_r = \"\"\n",
    "                size_digits = len(str(num_rows - 1))\n",
    "                while len(str(r)) < size_digits:\n",
    "                    l_r = l_r + \"0\"\n",
    "                    size_digits -= 1\n",
    "                l_r = l_r + str(r)\n",
    "                \n",
    "                l_c = \"\"\n",
    "                size_digits = len(str(num_cols - 1))\n",
    "                while len(str(c)) < size_digits:\n",
    "                    l_c = l_c + \"0\"\n",
    "                    size_digits -= 1\n",
    "                l_c = l_c + str(c)\n",
    "                \n",
    "                l = l_r + '-' + l_c\n",
    "                \n",
    "                cell = Cell(self, l, (r, c)) \n",
    "                self.cells.append(cell)\n",
    "                idx = idx + 1\n",
    "\n",
    "        # If not special cell statuses have been set set defaults\n",
    "        if special_cells == None:\n",
    "\n",
    "            self.cells[0].kind = 'start'\n",
    "            self.start_cell = self.cells[0]\n",
    "            self.cells[-1].kind = 'goal'\n",
    "            self.goal_cell = self.cells[-1]\n",
    "        \n",
    "        # If the user has set special cell statuses then add them\n",
    "        else:\n",
    "            start_added = False\n",
    "            goal_added = False\n",
    "            \n",
    "            # Special cells contains a list of ((row, col), kind) pairs for special cells\n",
    "            for entry in special_cells:\n",
    "                if entry[1] in self.allowed_special_cells:\n",
    "                    self.set_cell_kind(entry[0][0], entry[0][1], entry[1])\n",
    "                    \n",
    "                    # If the start cell has just been added then update it and record this\n",
    "                    if entry[1] == 'start':\n",
    "                        self.start_cell = self.find(entry[0][0], entry[0][1])\n",
    "                        start_added = True\n",
    "                        \n",
    "                    # If the goal cell has just been added then update it and record this\n",
    "                    if entry[1] == 'goal':\n",
    "                        self.goal_cell = self.find(entry[0][0], entry[0][1])\n",
    "                        goal_added = True\n",
    "                   \n",
    "            # If the start and goal haven't been added then add them as first and laast cell respectively\n",
    "            if not start_added:\n",
    "                self.cells[0].kind = 'start'\n",
    "                self.start_cell = self.cells[0]\n",
    "            if not goal_added:\n",
    "                self.cells[-1].kind = 'goal'\n",
    "                self.goal_cell = self.cells[-1]\n",
    "            \n",
    "        # Generate a list of vald action state pairs for each cell\n",
    "        for c in self.cells:\n",
    "            c.generate_action_state_pairs(self.rewards)\n",
    "        \n",
    "        # Generate a numeric version and label version of the world map - used for drawing worlds\n",
    "        self.generate_numeric_map()\n",
    "        self.generate_label_map()\n",
    "            \n",
    "    # Find a cell oobject based on a pair  of coordinates\n",
    "    def find(self, row, col):\n",
    "        \"\"\"Find a cell oobject based on a pair  of coordinates\"\"\"\n",
    "        cell = None\n",
    "        for c in self.cells: \n",
    "            if c.position == (row, col):\n",
    "                cell = c\n",
    "                break\n",
    "\n",
    "        return cell\n",
    "\n",
    "    # Set the kind of a cell given its row and col coords\n",
    "    def set_cell_kind(self, row, col, kind):\n",
    "        \"\"\"Set the kind of a cell given its row and col coords\"\"\"\n",
    "        cell = None\n",
    "        for c in self.cells: \n",
    "            if c.position == (row, col):\n",
    "                c.kind = kind\n",
    "                cell = c\n",
    "                break\n",
    "\n",
    "        return cell\n",
    "\n",
    "    # Find a cell object based on a label\n",
    "    def find_label(self, label):\n",
    "        \"\"\"Find a cell oobject based on a label\"\"\"\n",
    "        idx = 0\n",
    "        cell = None\n",
    "        for c in self.cells: \n",
    "            if c.label == label:\n",
    "                cell = c\n",
    "                break\n",
    "\n",
    "        return cell\n",
    "    \n",
    "    # Set the coordinates of the goal cell\n",
    "    def set_goal(self, row, col):\n",
    "        \"\"\"Find a cell object based on a label\"\"\"\n",
    "        self.goal_cell = find(row, col)\n",
    "        \n",
    "    # Print an ascii version of the the world environment    \n",
    "    def print_world(self, details = False):\n",
    "        \"\"\"Print an ascii version of the the world environment\n",
    "        Arguments\n",
    "            details: True gies a details version of each cell whereas False gives a simple grid version\n",
    "        \"\"\"\n",
    "        print(\"Goal: \", self.goal_cell.to_string())\n",
    "        \n",
    "        if details:\n",
    "\n",
    "            for idx, c in enumerate(self.cells):\n",
    "                print(c.to_string(True))\n",
    "                if(idx%5 == 0):\n",
    "                    print(\"\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            idx = 0\n",
    "            for r in range(0, self.num_rows):\n",
    "                print(\"|\" + \"---\"*self.num_cols + \"|\")\n",
    "                line = \"|\"\n",
    "                for c in range(0, self.num_cols):\n",
    "                    line = line + self.cells[idx].label + \"|\"\n",
    "                    idx += 1\n",
    "                print(line)\n",
    "            print(\"|\" + \"---\"*self.num_cols + \"|\")\n",
    "        \n",
    "    # Generate a numeric array version of the world - used for drawing a heatmap based map image\n",
    "    def generate_numeric_map(self):\n",
    "        \"\"\"Generate a numeric array version of the world - used for drawing a heatmap based map image\"\"\"\n",
    "        self.numeric_map = np.zeros((self.num_rows, self.num_cols))\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                value = 0\n",
    "            elif c.kind == 'trap':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap1':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap2':\n",
    "                value = 20\n",
    "            elif c.kind == 'trap3':\n",
    "                value = 30\n",
    "            elif c.kind == 'start':\n",
    "                value = 40\n",
    "            elif c.kind == 'goal':\n",
    "                value = 50\n",
    "            else:\n",
    "                value = 0\n",
    "                \n",
    "            self.numeric_map[c.position[0], c.position[1]] = value\n",
    "\n",
    "    # Generate a numeric array version of the world - used for drawing a heatmap based map image\n",
    "    def generate_label_map(self):\n",
    "        \"\"\"Generate a numeric array version of the world - used for drawing a heatmap based map image\"\"\"\n",
    "        self.label_map = np.empty((self.num_rows, self.num_cols), dtype= np.str)\n",
    "\n",
    "        # Unicode arrows http://xahlee.info/comp/unicode_arrows.html\n",
    "        # left: 2190\n",
    "        # right: 2192\n",
    "        # up: 2191\n",
    "        # down: 2193\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                label = ''\n",
    "            elif c.kind == 'trap':\n",
    "                label = 't'\n",
    "            elif c.kind == 'trap1':\n",
    "                label = 'm'\n",
    "            elif c.kind == 'trap2':\n",
    "                label = 't'\n",
    "            elif c.kind == 'trap3':\n",
    "                label = 'f'\n",
    "            elif c.kind == 'start':\n",
    "                label = 'S'\n",
    "            elif c.kind == 'goal':\n",
    "                label = 'G'\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "            self.label_map[c.position[0], c.position[1]] = label\n",
    "            \n",
    "    # Draw the world map colour coding special cells\n",
    "    def draw_map(self):\n",
    "        \"\"\"Draw the world map colour coding special cells\"\"\"\n",
    "        # Set up a colour map for different cell types (0 = 'empty', 10 = 'trap', 20 = 'start', 30 = 'goal')\n",
    "        #colors = [\"light grey\", \"yellow\", \"orange\", \"red\", \"faded green\", \"ochre\"]\n",
    "        colors = [\"light grey\", \"dark grey\", \"dark grey\", \"dark grey\", \"black\", \"black\"]\n",
    "        ax = sns.heatmap(self.numeric_map, annot = self.label_map, fmt='', linewidths=.5, vmax= 50, vmin = 0, cmap=sns.xkcd_palette(colors), cbar=False) \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "        return ax\n",
    "    \n",
    "\n",
    "            \n",
    "    # Draw the world map colour coding special cells\n",
    "    def draw_map_visited_states(self, states, filename = None):\n",
    "        \"\"\"Draw the world map colour coding special cells and cells visited on a trip\"\"\"\n",
    "\n",
    "        # gGenerate a new numeric map showing visited cells\n",
    "        new_numeric_map = np.zeros((self.num_rows, self.num_cols))\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                value = 0\n",
    "            elif c.kind == 'trap':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap1':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap2':\n",
    "                value = 20\n",
    "            elif c.kind == 'trap3':\n",
    "                value = 30\n",
    "            elif c.kind == 'start':\n",
    "                value = 40\n",
    "            elif c.kind == 'goal':\n",
    "                value = 50\n",
    "            else:\n",
    "                value = 0\n",
    "                \n",
    "            if c.label in states and c.label != self.start_cell.label and c.label != self.goal_cell.label:\n",
    "                value = 60\n",
    "                \n",
    "            new_numeric_map[c.position[0], c.position[1]] = value\n",
    "            \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Set up a colour map for different cell types (0 = 'empty', 10 = 'trap', 20 = 'start', 30 = 'goal')\n",
    "        #colors = [\"light grey\", \"yellow\", \"orange\", \"red\", \"faded green\", \"ochre\", \"black\"]\n",
    "        colors = [\"light grey\", \"dark grey\", \"dark grey\", \"dark grey\", \"black\", \"black\", \"white\"]\n",
    "        ax = sns.heatmap(new_numeric_map, annot = self.label_map, fmt='', linewidths=.5, vmax= 60, vmin = 0, cmap=sns.xkcd_palette(colors), cbar=False) \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            \n",
    "        if(filename != None):\n",
    "            plt.savefig(filename, bbox_inches = 'tight', dpi = saving_dpi)\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    # Draw the world map colour coding special cells\n",
    "    def draw_map_visited_states_labels(self, states, actions, filename = None):\n",
    "        \"\"\"Draw the world map colour coding special cells and cells visited on a trip\"\"\"\n",
    "\n",
    "        # gGenerate a new numeric map showing visited cells\n",
    "        new_numeric_map = np.zeros((self.num_rows, self.num_cols))\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                value = 0\n",
    "            elif c.kind == 'trap':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap1':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap2':\n",
    "                value = 20\n",
    "            elif c.kind == 'trap3':\n",
    "                value = 30\n",
    "            elif c.kind == 'start':\n",
    "                value = 40\n",
    "            elif c.kind == 'goal':\n",
    "                value = 50\n",
    "            else:\n",
    "                value = 0\n",
    "          \n",
    "            new_numeric_map[c.position[0], c.position[1]] = value\n",
    "\n",
    "        # Unicode arrows http://xahlee.info/comp/unicode_arrows.html\n",
    "        # left: 2190\n",
    "        # right: 2192\n",
    "        # up: 2191\n",
    "        # down: 2193\n",
    "        \n",
    "        new_label_map = np.empty((self.num_rows, self.num_cols), dtype= np.str)\n",
    "                \n",
    "        self.find_label\n",
    "        for idx, c_label in enumerate(states): \n",
    "\n",
    "            c = self.find_label(c_label)\n",
    "            \n",
    "            # The last state has no action, so for all other states find the action and draw it\n",
    "            if idx != (len(states)-1):\n",
    "                \n",
    "                if actions[idx] == 'left':\n",
    "                    label = u'\\u2190'\n",
    "                    #label = u'\\u21e6'\n",
    "                elif actions[idx] == 'right':\n",
    "                    label = u'\\u2192'\n",
    "                    #label = u'\\u21e8'\n",
    "                elif actions[idx] == 'up':\n",
    "                    label = u'\\u2191'\n",
    "                    #label = u'\\u21e7'\n",
    "                elif actions[idx] == 'down':\n",
    "                    label = u'\\u2193'\n",
    "                    #label = u'\\u21e9'\n",
    "                #else:\n",
    "                #    label = 0\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "            new_label_map[c.position[0], c.position[1]] = label\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Set up a colour map for different cell types (0 = 'empty', 10 = 'trap', 20 = 'start', 30 = 'goal')\n",
    "        #colors = [\"light grey\", \"yellow\", \"orange\", \"red\", \"faded green\", \"ochre\", \"black\"]\n",
    "        colors = [\"light grey\", \"dark grey\", \"dark grey\", \"dark grey\", \"black\", \"black\"]\n",
    "        sns.set(font_scale=1.5)\n",
    "        ax = sns.heatmap(new_numeric_map, annot = new_label_map, fmt='', linewidths=.5, vmax= 60, vmin = 0, cmap=sns.xkcd_palette(colors), cbar=False) \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            \n",
    "        if(filename != None):\n",
    "            plt.savefig(filename, bbox_inches = 'tight', dpi = saving_dpi)\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    # Draw the world map colour coding special cells\n",
    "    def draw_map_visited_states_labels(self, states, actions, filename = None):\n",
    "        \"\"\"Draw the world map colour coding special cells and cells visited on a trip\"\"\"\n",
    "\n",
    "        # gGenerate a new numeric map showing visited cells\n",
    "        new_numeric_map = np.zeros((self.num_rows, self.num_cols))\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                value = 0\n",
    "            elif c.kind == 'trap':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap1':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap2':\n",
    "                value = 20\n",
    "            elif c.kind == 'trap3':\n",
    "                value = 30\n",
    "            elif c.kind == 'start':\n",
    "                value = 40\n",
    "            elif c.kind == 'goal':\n",
    "                value = 50\n",
    "            else:\n",
    "                value = 0\n",
    "          \n",
    "            new_numeric_map[c.position[0], c.position[1]] = value\n",
    "\n",
    "        # Unicode arrows http://xahlee.info/comp/unicode_arrows.html\n",
    "        # left: 2190\n",
    "        # right: 2192\n",
    "        # up: 2191\n",
    "        # down: 2193\n",
    "        \n",
    "        new_label_map = np.empty((self.num_rows, self.num_cols), dtype= np.str)\n",
    "                \n",
    "        self.find_label\n",
    "        for idx, c_label in enumerate(states): \n",
    "\n",
    "            c = self.find_label(c_label)\n",
    "            \n",
    "            # The last state has no action, so for all other states find the action and draw it\n",
    "            if idx != (len(states)-1):\n",
    "                \n",
    "                if actions[idx] == 'left':\n",
    "                    label = u'\\u2190'\n",
    "                    #label = u'\\u21e6'\n",
    "                elif actions[idx] == 'right':\n",
    "                    label = u'\\u2192'\n",
    "                    #label = u'\\u21e8'\n",
    "                elif actions[idx] == 'up':\n",
    "                    label = u'\\u2191'\n",
    "                    #label = u'\\u21e7'\n",
    "                elif actions[idx] == 'down':\n",
    "                    label = u'\\u2193'\n",
    "                    #label = u'\\u21e9'\n",
    "                \n",
    "                new_label_map[c.position[0], c.position[1]] = label\n",
    "            \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Set up a colour map for different cell types (0 = 'empty', 10 = 'trap', 20 = 'start', 30 = 'goal')\n",
    "        #colors = [\"light grey\", \"yellow\", \"orange\", \"red\", \"faded green\", \"ochre\", \"black\"]\n",
    "        colors = [\"light grey\", \"dark grey\", \"dark grey\", \"dark grey\", \"black\", \"black\"]\n",
    "        ax = sns.heatmap(new_numeric_map, annot = new_label_map, fmt='', linewidths=.5, vmax= 60, vmin = 0, cmap=sns.xkcd_palette(colors), cbar=False, annot_kws={\"size\": 20}) \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            \n",
    "        if(filename != None):\n",
    "            plt.savefig(filename, bbox_inches = 'tight', dpi = saving_dpi)\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "\n",
    "    # Draw the world map colour coding special cells\n",
    "    def draw_map_policy(self, agent, filename = None):\n",
    "        \"\"\"Draw the world map colour coding special cells and showing the best action from each state\"\"\"\n",
    "\n",
    "        # gGenerate a new numeric map showing visited cells\n",
    "        new_numeric_map = np.zeros((self.num_rows, self.num_cols))\n",
    "        \n",
    "        for idx, c in enumerate(self.cells):\n",
    "            if c.kind == 'empty':\n",
    "                value = 0\n",
    "            elif c.kind == 'trap':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap1':\n",
    "                value = 10\n",
    "            elif c.kind == 'trap2':\n",
    "                value = 20\n",
    "            elif c.kind == 'trap3':\n",
    "                value = 30\n",
    "            elif c.kind == 'start':\n",
    "                value = 40\n",
    "            elif c.kind == 'goal':\n",
    "                value = 50\n",
    "            else:\n",
    "                value = 0\n",
    "          \n",
    "            new_numeric_map[c.position[0], c.position[1]] = value\n",
    "\n",
    "        # Unicode arrows http://xahlee.info/comp/unicode_arrows.html\n",
    "        # left: 2190\n",
    "        # right: 2192\n",
    "        # up: 2191\n",
    "        # down: 2193\n",
    "        \n",
    "        new_label_map = np.empty((self.num_rows, self.num_cols), dtype= np.str)\n",
    "                \n",
    "        for idx, c in enumerate(self.cells):\n",
    "\n",
    "            if c != self.goal_cell:\n",
    "                a = agent.choose_action_greedy_for_cell(c)\n",
    "\n",
    "                if a == 'left':\n",
    "                    label = u'\\u2190'\n",
    "                    #label = u'\\u21e6'\n",
    "                elif a == 'right':\n",
    "                    label = u'\\u2192'\n",
    "                    #label = u'\\u21e8'\n",
    "                elif a == 'up':\n",
    "                    label = u'\\u2191'\n",
    "                    #label = u'\\u21e7'\n",
    "                elif a == 'down':\n",
    "                    label = u'\\u2193'\n",
    "                    #label = u'\\u21e9'\n",
    "\n",
    "                new_label_map[c.position[0], c.position[1]] = label\n",
    "            \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Set up a colour map for different cell types (0 = 'empty', 10 = 'trap', 20 = 'start', 30 = 'goal')\n",
    "        #colors = [\"light grey\", \"yellow\", \"orange\", \"red\", \"faded green\", \"ochre\", \"black\"]\n",
    "        colors = [\"light grey\", \"dark grey\", \"dark grey\", \"dark grey\", \"black\", \"black\"]\n",
    "        ax = sns.heatmap(new_numeric_map, annot = new_label_map, fmt='', linewidths=.5, vmax= 60, vmin = 0, cmap=sns.xkcd_palette(colors), cbar=False, annot_kws={\"size\": 20}) \n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            \n",
    "        if(filename != None):\n",
    "            plt.savefig(filename, bbox_inches = 'tight', dpi = saving_dpi)\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = World()\n",
    "plt.figure(figsize=(5, 5))\n",
    "w.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = World(8, 8, special_cells=[((4, 1), 'trap'), ((4, 2), 'trap'), ((4, 3), 'trap'), ((4, 4), 'trap'), ((4, 5), 'trap'), ((4, 6), 'trap')])\n",
    "plt.figure(figsize=(5, 5))\n",
    "w.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = World(7, 7, special_cells=[((0, 3), 'start'), ((6, 3), 'goal'),\n",
    "                               ((2, 1), 'trap1'), ((2, 2), 'trap3'), ((2, 4), 'trap3'), ((2, 5), 'trap1'), \n",
    "                               ((3, 1), 'trap1'), ((3, 2), 'trap3'), ((3, 4), 'trap3'), ((3, 5), 'trap1'), \n",
    "                               ((4, 1), 'trap1'), ((4, 2), 'trap3'), ((4, 4), 'trap3'), ((4, 5), 'trap1')])\n",
    "plt.figure(figsize=(5, 5))\n",
    "w.draw_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Agent class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility fucntion to creaate empty data frame with column names and types\n",
    "# From: https://stackoverflow.com/questions/36462257/create-empty-dataframe-in-pandas-specifying-column-types\n",
    "def df_empty(columns, dtypes, index=None):\n",
    "    assert len(columns)==len(dtypes)\n",
    "    df = pd.DataFrame(index=index)\n",
    "    for c,d in zip(columns, dtypes):\n",
    "        df[c] = pd.Series(dtype=d)\n",
    "    return df\n",
    "\n",
    "df = df_empty(['a', 'b'], dtypes=[np.int64, np.int64])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_agent:\n",
    "    \n",
    "    def __init__(self, world, start, kind = \"SARSA\", epsilon = 0.1, alpha = 0.2, gamma = 0.9):\n",
    "        \"\"\"Constructor\n",
    "        Arguments\n",
    "            world: the world to which the agent belongs\n",
    "            start: the cell to which the agent belongs when it starts\n",
    "            epsilon: epsilon parameter for epsilon greedy action selection policy\n",
    "            alpha: learning rate for SARSA algorithm\n",
    "            gamma: gamma parameter to balance reward and state contributions in SARSA algorithm\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "        self.position = start\n",
    "        self.kind = kind\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.action_value_function_table = None\n",
    "        self.pandas_action_value_function_table = None\n",
    "        self.heatmaps = dict()\n",
    "\n",
    "        self.__generate_action_value_function_table()\n",
    "            \n",
    "        self.__generate_pandas_action_value_function_table()\n",
    "        self.__generate_action_value_function_heatmaps()\n",
    "        \n",
    "    # Construct the acttion value funtion table. Iterate thtrough all state-action pairs and assignment them random values.\n",
    "    def __generate_action_value_function_table(self):\n",
    "        \"\"\"Construct the acttion value funtion table. Iterate thtrough all state-action pairs and assignment them random values.\"\"\"\n",
    "        self.action_value_function_table = dict()\n",
    "    \n",
    "        # Iterate through states\n",
    "        for c in self.world.cells:\n",
    "            \n",
    "            # Iterate through actions and assign each state-action pair a random values\n",
    "            actions = dict()\n",
    "            for a in c.actions.keys():\n",
    "    \n",
    "                actions[a] = random.uniform(-1, 1)\n",
    "            \n",
    "                # Set the value of any actions from the gaol table  to  0\n",
    "                if(c.kind == 'goal'):\n",
    "                    actions[a] = 0\n",
    "                    \n",
    "            self.action_value_function_table[c.label] = actions\n",
    "\n",
    "            # Construct the acttion value funtion table. Iterate thtrough all state-action pairs and assignment them random values.\n",
    "\n",
    "    def __generate_pandas_action_value_function_table(self):\n",
    "        \"\"\"Construct a pandas version of the action value funtion table.\"\"\"\n",
    "        self.pandas_action_value_function_table = df_empty(['State', 'Action', 'Value'], dtypes=[np.str, np.str, np.float])\n",
    "        \n",
    "        row_count = 0\n",
    "        for s in self.action_value_function_table.keys():\n",
    "            for a in self.action_value_function_table[s].keys():\n",
    "                self.pandas_action_value_function_table.loc[row_count] = [s, a, self.action_value_function_table[s][a]]\n",
    "                row_count = row_count + 1\n",
    "            \n",
    "    # Print a text version of the action vbalue function table\n",
    "    def print_action_value_function_table(self):\n",
    "        \"\"\"Print a text version of the action vbalue function table\"\"\"\n",
    "        for s in self.action_value_function_table.keys():\n",
    "            for a in self.action_value_function_table[s].keys():\n",
    "                print(s + \" \" + a + \" \" + \" \" + str(self.action_value_function_table[s][a]))\n",
    "\n",
    "    # Write a version of the action value table to csv file\n",
    "    def save_csv_action_value_function_table(self, out_filename):\n",
    "        \"\"\"Write a version of the action value table to a csv file\"\"\"\n",
    "        with open(out_filename, 'w') as out_file:\n",
    "            out_file.write(\"State,Action,Value\\n\")\n",
    "            for s in self.action_value_function_table.keys():\n",
    "                for a in self.action_value_function_table[s].keys():\n",
    "                    line = s + \",\" + a + \",\" + str(self.action_value_function_table[s][a]) + \" \\n\"\n",
    "                    out_file.write(line)\n",
    "\n",
    "    # Write a version of the action value table to csv file\n",
    "    def save_latex_action_value_function_table(self, out_filename):\n",
    "        \"\"\"Write a version of the action value table to a latex file with FMLPDA formatting\"\"\"\n",
    "        with open(out_filename, 'w') as out_file:\n",
    "            out_file.write('State & Action & Value \\\\\\\\ \\n')\n",
    "            for s in self.action_value_function_table.keys():\n",
    "                for a in self.action_value_function_table[s].keys():\n",
    "                    line = \"\\\\rlState{\" + s + \"} & \\\\rlAction{\" + a + \"} & $\" + str(round(self.action_value_function_table[s][a], 3)) + \"$ \\\\\\\\ \\n\"\n",
    "                    out_file.write(line)\n",
    "        \n",
    "    # Generate a set of numeric heatmap represenations for each action. Used for drawing things.\n",
    "    def __generate_action_value_function_heatmaps(self):\n",
    "        \"\"\"Generate a set of numeric heatmap represenations for each action. Used for drawing things.\"\"\"\n",
    "\n",
    "        # Intialise empty heat mapes \n",
    "        self.heatmaps = dict()\n",
    "        self.heatmaps['left'] = np.zeros((self.world.num_rows, self.world.num_cols))\n",
    "        self.heatmaps['right'] = np.zeros((self.world.num_rows, self.world.num_cols))\n",
    "        self.heatmaps['up'] = np.zeros((self.world.num_rows, self.world.num_cols))\n",
    "        self.heatmaps['down'] = np.zeros((self.world.num_rows, self.world.num_cols))\n",
    "\n",
    "        # Iterate through the action value function table and fill up heatmaps\n",
    "        for s in self.action_value_function_table.keys():\n",
    "            for a in self.action_value_function_table[s].keys():\n",
    "                c = self.world.find_label(s)\n",
    "                self.heatmaps[a][c.position[0], c.position[1]] = self.action_value_function_table[s][a]\n",
    "\n",
    "    # Draw the heatmaps in a nice aragnemnt with a world mpa in the middle\n",
    "    def draw_action_value_function_heatmaps(self, layout = 'grid', color = 'RGB', filename = None):\n",
    "        \"\"\"Draw the action value function heatmaps in a nice aragnemnt with a world map in the middle\"\"\"\n",
    "        \n",
    "        # Update teh action value heatmap data structures\n",
    "        self.__generate_action_value_function_heatmaps()\n",
    "\n",
    "        if color == 'RGB':\n",
    "            cmap_used = sns.color_palette(\"BrBG\", 21)\n",
    "        elif color == 'BW':\n",
    "            cmap_used = sns.color_palette(\"Greys\", 12)\n",
    "        else:\n",
    "            cmap_used = sns.color_palette(\"BrBG\", 21)\n",
    "            \n",
    "        #max_value = max(50, self.heatmaps['up'].max(), self.heatmaps['down'].max(), self.heatmaps['left'].max(), self.heatmaps['right'].max())\n",
    "        #min_value = min(-20, self.heatmaps['up'].min(), self.heatmaps['down'].min(), self.heatmaps['left'].min(), self.heatmaps['right'].min())\n",
    "        max_value = 50\n",
    "        min_value = -50\n",
    "        \n",
    "        if(layout == \"grid\"):            \n",
    "            plt.figure(figsize=(12,12))\n",
    "\n",
    "            plt.subplot(332)\n",
    "            ax = sns.heatmap(self.heatmaps['up'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False) \n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('up')\n",
    "            plt.subplot(334)\n",
    "            ax = sns.heatmap(self.heatmaps['left'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False) \n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('left')\n",
    "            plt.subplot(335)\n",
    "            ax = self.world.draw_map()\n",
    "            ax.set_title('world')\n",
    "            plt.subplot(336)\n",
    "            ax = sns.heatmap(self.heatmaps['right'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=True)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('right')\n",
    "            plt.subplot(338)\n",
    "            ax = sns.heatmap(self.heatmaps['down'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('down')\n",
    "\n",
    "        elif(layout == 'line'):\n",
    "            fig = plt.figure(figsize=(20,4))\n",
    "            cbar_ax = fig.add_axes([.91, .15, .015, .7])\n",
    "            plt.subplot(151)\n",
    "            ax = self.world.draw_map()\n",
    "            ax.set_title('world')\n",
    "            plt.subplot(152)\n",
    "            ax = sns.heatmap(self.heatmaps['up'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False) \n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('up')\n",
    "            plt.subplot(153)\n",
    "            ax = sns.heatmap(self.heatmaps['down'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('down')\n",
    "            plt.subplot(154)\n",
    "            ax = sns.heatmap(self.heatmaps['left'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=False) \n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('left')\n",
    "            plt.subplot(155)\n",
    "            ax = sns.heatmap(self.heatmaps['right'], linewidths=.5, vmin = min_value, vmax= max_value, center= 0, cmap=cmap_used, cbar=True, cbar_ax = cbar_ax)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "            ax.set_title('right')\n",
    "            \n",
    "        if(filename != None):\n",
    "            plt.savefig(filename, bbox_inches = 'tight', dpi = saving_dpi)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    # Choose an action using the greedy strategy - best available\n",
    "    def choose_action_greedy(self, verbose = 0):\n",
    "        \"\"\"Choose an action using the greedy strategy - best available\"\"\"\n",
    "        potential_actions = list(self.position.actions.keys())\n",
    "        \n",
    "        # Choose the best action\n",
    "        action = potential_actions[0]\n",
    "        #max_value = self.action_value_function_table[self.position.label][action]['value']\n",
    "        max_value = self.action_value_function_table[self.position.label][action]\n",
    "\n",
    "        for a in potential_actions[1:]:\n",
    "            #value = self.action_value_function_table[self.position.label][a]['value']\n",
    "            value = self.action_value_function_table[self.position.label][a]\n",
    "            if value > max_value:\n",
    "                action = a\n",
    "                max_value = value\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    # Choose an action using the greedy strategy - best available\n",
    "    def choose_action_greedy_for_cell(self, cell, verbose = 0):\n",
    "        \"\"\"Choose an action using the greedy strategy - best available\"\"\"\n",
    "        potential_actions = list(cell.actions.keys())\n",
    "        \n",
    "        # Choose the best action\n",
    "        action = potential_actions[0]\n",
    "        #max_value = self.action_value_function_table[self.position.label][action]['value']\n",
    "        max_value = self.action_value_function_table[cell.label][action]\n",
    "\n",
    "        for a in potential_actions[1:]:\n",
    "            #value = self.action_value_function_table[self.position.label][a]['value']\n",
    "            value = self.action_value_function_table[cell.label][a]\n",
    "            if value > max_value:\n",
    "                action = a\n",
    "                max_value = value\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    # Choose an action using an epsilon greedy strategy\n",
    "    def choose_action_e_greedy(self, verbose = 0):\n",
    "        \"\"\"Choose an action using an epsilon greedy strategy\"\"\"\n",
    "        potential_actions = list(self.position.actions.keys())\n",
    "        \n",
    "        # Generate a random number, if less than epsilon do a random acrtion, otherwise pick the action with the highest value\n",
    "        r = random.uniform(0, 1)\n",
    "\n",
    "        if(verbose == 2): print(\"choose_action_e_greedy: \", r)\n",
    "        if(r < self.epsilon):\n",
    "            if(verbose == 2): print(\"random\")\n",
    "            action = random.choice(potential_actions)\n",
    "\n",
    "        # If greater than epsilon choose the best action\n",
    "        else:\n",
    "            if(verbose == 2): print(\"greedy\")\n",
    "            action = self.choose_action_greedy()\n",
    "            \n",
    "        return action\n",
    "\n",
    "    # Choose an action following the current policy\n",
    "    def choose_action_on_policy(self, verbose = 0):\n",
    "        \"\"\"Choose an action following the current policy\"\"\"\n",
    "        return self.choose_action_e_greedy(verbose)\n",
    "    \n",
    "    # Choose an action off policy - choose the best one\n",
    "    def choose_action_off_policy(self, verbose = 0):\n",
    "        \"\"\"Choose an action following the current policy\"\"\"\n",
    "        return self.choose_action_greedy(verbose)\n",
    "    \n",
    "    # Make  a move and return the resulting reward and the new state\n",
    "    def move(self, action):\n",
    "        \"\"\"Make  a move and return the resulting reward and the new state\"\"\"\n",
    "        r = self.position.actions[action]['reward']\n",
    "        s_prime = self.position.actions[action]['state']\n",
    "                \n",
    "        self.position = s_prime\n",
    "        return r, s_prime\n",
    "    \n",
    "    # Perform a number of reinforcement learning episodes \n",
    "    def perform_RL_episodes(self, num_episodes = 1, heatmap_layout = 'grid', image_filename_root = None, verbose = 0):\n",
    "        \"\"\"Perform a number of reinforcement learning episodes using whicherver approach the agent is setup for\n",
    "        Arguments\n",
    "            num_episodes: the number of RL episodes to perform\n",
    "            verbose: how much debug info to print (0: none, 1: updates every 50 episodes, 2: updates every episode])\n",
    "        \"\"\"\n",
    "        if self.kind == \"SARSA\":\n",
    "            return self.perform_RL_episodes_SARSA(num_episodes, heatmap_layout, image_filename_root, verbose)\n",
    "    \n",
    "        elif self.kind == \"Q_Learning\":\n",
    "            return self.perform_RL_episodes_Q_Learning(num_episodes, heatmap_layout, image_filename_root, verbose)\n",
    "    \n",
    "\n",
    "    # Perform a number of reinforcement learning episodes using the SARSA approach\n",
    "    def perform_RL_episodes_SARSA(self, num_episodes = 1, heatmap_layout = 'grid', image_filename_root = None, verbose = 0):\n",
    "        \"\"\"Perform a number of reinforcement learning episodes using the SARSA approach\n",
    "        Arguments\n",
    "            num_episodes: the number of RL episodes to perform\n",
    "            verbose: how much debug info to print (0: none, 1: updates every 50 episodes, 2: updates every episode])\n",
    "        \"\"\"\n",
    "        episodes = list()\n",
    "\n",
    "        # Iterate through the episodes\n",
    "        for e in range(0, num_episodes):\n",
    "            \n",
    "            total_reward = 0\n",
    "            moves = list()\n",
    "            states = list()\n",
    "            \n",
    "            # Reset agent to first state\n",
    "            self.position = self.world.start_cell\n",
    "\n",
    "            s = self.position\n",
    "            \n",
    "            a = self.choose_action_on_policy(verbose)\n",
    "\n",
    "            # Repeat until the goal position is reached\n",
    "            idx = 0\n",
    "            while s.position != self.world.goal_cell.position:\n",
    "\n",
    "                # Take a move\n",
    "                r, s_prime = self.move(a)\n",
    "                total_reward += r\n",
    "                moves.append(a)\n",
    "                states.append(s)\n",
    "\n",
    "                # Choose the next action\n",
    "                a_prime = self.choose_action_on_policy(verbose)\n",
    "\n",
    "                # Calcualte the new value for the action value function\n",
    "                q_s_a = self.action_value_function_table[s.label][a]\n",
    "                q_s_prime_a_prime = self.action_value_function_table[s_prime.label][a_prime]\n",
    "                new_value = q_s_a + self.alpha*(r + self.gamma*q_s_prime_a_prime - q_s_a)\n",
    "\n",
    "                # Update the value table\n",
    "                self.action_value_function_table[s.label][a] =  new_value\n",
    "\n",
    "                # Update the current state and ation\n",
    "                s = s_prime\n",
    "                a = a_prime\n",
    "                idx = idx + 1\n",
    "            \n",
    "            states.append(s)\n",
    "            episodes.append({'idx':e,'return':total_reward, 'moves':moves, 'states':states})\n",
    "            \n",
    "            # Draw heatmaps for every 50th episode\n",
    "            if((verbose == 1 and e%50 == 0) or verbose == 2):\n",
    "                print(\"Episode:\", e)\n",
    "                print(\"Moves:\",  moves)\n",
    "                print(\"Reward: \", total_reward) \n",
    "                filename = None\n",
    "                if(image_filename_root != None):\n",
    "                    filename = image_filename_root + \"_ep_\" + str(e) + \".pdf\"\n",
    "                self.draw_action_value_function_heatmaps(heatmap_layout, filename)\n",
    "\n",
    "                filename = None\n",
    "                if(image_filename_root != None):\n",
    "                    filename = image_filename_root + \"_ep_\" + str(e) + \"_policy.pdf\"\n",
    "                self.world.draw_map_policy(self, filename)\n",
    "                \n",
    "                if(image_filename_root != None):\n",
    "                    filename_tex = image_filename_root + \"_ep_\" + str(e) + \"_action_value_table.tex\"\n",
    "                    self.save_latex_action_value_function_table(filename_tex)\n",
    "                    filename_csv = image_filename_root + \"_ep_\" + str(e) + \"_action_value_table.csv\"\n",
    "                    self.save_csv_action_value_function_table(filename_csv)\n",
    "\n",
    "                \n",
    "       \n",
    "        # Dreaw heatmaps of the final table\n",
    "        print(\"Episode:\", e)\n",
    "        print(\"Moves:\",  moves)\n",
    "        print(\"Reward: \", total_reward) \n",
    "        filename = None\n",
    "        if(image_filename_root != None):\n",
    "            filename = image_filename_root + \"_ep_\" + str(e) + \".pdf\"\n",
    "        self.draw_action_value_function_heatmaps(heatmap_layout, filename)\n",
    "        filename = None\n",
    "        if(image_filename_root != None):\n",
    "            filename = image_filename_root + \"_ep_\" + str(e) + \"_policy.pdf\"\n",
    "        self.world.draw_map_policy(self, filename)\n",
    "        \n",
    "        return episodes\n",
    "    \n",
    "    # Perform a number of reinforcement learning episodes using the Q Learning approach\n",
    "    def perform_RL_episodes_Q_Learning(self, num_episodes = 1, heatmap_layout = 'grid', image_filename_root = None, verbose = 0):\n",
    "        \"\"\"Perform a number of reinforcement learning episodes using the Q Learning approach\n",
    "        Arguments\n",
    "            num_episodes: the number of RL episodes to perform\n",
    "            verbose: how much debug info to print (0: none, 1: updates every 50 episodes, 2: updates every episode])\n",
    "        \"\"\"\n",
    "        episodes = list()\n",
    "\n",
    "        # Iterate through the episodes\n",
    "        for e in range(0, num_episodes):\n",
    "            \n",
    "            total_reward = 0\n",
    "            moves = list()\n",
    "            states = list()\n",
    "\n",
    "            # Reset agent to first state\n",
    "            self.position = self.world.start_cell\n",
    "\n",
    "            s = self.position\n",
    "            \n",
    "            idx = 0\n",
    "            # Repeat until the goal position is reached\n",
    "            while s.position != self.world.goal_cell.position:\n",
    "\n",
    "                a = self.choose_action_on_policy(verbose)\n",
    "                    \n",
    "                # Take a move\n",
    "                r, s_prime = self.move(a)\n",
    "                total_reward += r\n",
    "                moves.append(a)\n",
    "                states.append(s)\n",
    "                    \n",
    "                # Choose the next action\n",
    "                a_prime = self.choose_action_off_policy(verbose)\n",
    "\n",
    "                # Calcualte the new value for the action value function\n",
    "                q_s_a = self.action_value_function_table[s.label][a]\n",
    "                q_s_prime_a_prime = self.action_value_function_table[s_prime.label][a_prime]\n",
    "                new_value = q_s_a + self.alpha*(r + self.gamma*q_s_prime_a_prime - q_s_a)\n",
    "\n",
    "                # Update the value table\n",
    "                self.action_value_function_table[s.label][a] =  new_value\n",
    "\n",
    "                # Update the current state and ation\n",
    "                s = s_prime\n",
    "                \n",
    "                idx = idx + 1\n",
    "                \n",
    "            states.append(s)\n",
    "            episodes.append({'idx':e,'return':total_reward, 'moves':moves, 'states':states})\n",
    "            \n",
    "            # Draw heatmaps for every 50th episode\n",
    "            if((verbose == 1 and e%50 == 0) or verbose == 2):\n",
    "                print(\"Episode:\", e)\n",
    "                print(\"Moves:\",  moves)\n",
    "                print(\"Reward: \", total_reward) \n",
    "                filename = None\n",
    "                if(image_filename_root != None):\n",
    "                    filename = image_filename_root + \"_ep_\" + str(e) + \".pdf\"\n",
    "                self.draw_action_value_function_heatmaps(heatmap_layout, filename)\n",
    "                filename = None\n",
    "                if(image_filename_root != None):\n",
    "                    filename = image_filename_root + \"_ep_\" + str(e) + \"_policy.pdf\"\n",
    "                self.world.draw_map_policy(self, filename)\n",
    "                if(image_filename_root != None):\n",
    "                    filename_tex = image_filename_root + \"_ep_\" + str(e) + \"_action_value_table.tex\"\n",
    "                    self.save_latex_action_value_function_table(filename_tex)\n",
    "                    filename_csv = image_filename_root + \"_ep_\" + str(e) + \"_action_value_table.csv\"\n",
    "                    self.save_csv_action_value_function_table(filename_csv)\n",
    "\n",
    "       \n",
    "        # Dreaw heatmaps of the final table\n",
    "        print(\"Episode:\", e)\n",
    "        print(\"Moves:\",  moves)\n",
    "        print(\"Reward: \", total_reward) \n",
    "        if(image_filename_root != None):\n",
    "            filename = image_filename_root + \"_ep_\" + str(e) + \".pdf\"\n",
    "        self.draw_action_value_function_heatmaps(heatmap_layout, filename)\n",
    "        filename = None\n",
    "        if(image_filename_root != None):\n",
    "            filename = image_filename_root + \"_ep_\" + str(e) + \"_policy.pdf\"\n",
    "        self.world.draw_map_policy(self, filename)\n",
    "        \n",
    "        return episodes\n",
    "    \n",
    "    # Perform a number of reinforcement learning episodes using the Q Learning approach\n",
    "    def perform_offline_task(self, image_filename_root = None, verbose = 0):\n",
    "        \"\"\"Perform athe task using a trained value fucntion\n",
    "        Arguments\n",
    "            verbose: how much debug info to print (0: none, 1: updates every 50 episodes, 2: updates every episode])\n",
    "        \"\"\"\n",
    "            \n",
    "        total_reward = 0\n",
    "        moves = list()\n",
    "        states = list()\n",
    "\n",
    "        # Reset agent to first state\n",
    "        self.position = self.world.start_cell\n",
    "\n",
    "        s = self.position\n",
    "        states.append(s.label)\n",
    "            \n",
    "        idx = 0\n",
    "        # Repeat until the goal position is reached\n",
    "        while s.position != self.world.goal_cell.position:\n",
    "\n",
    "            a = self.choose_action_greedy(verbose)\n",
    "\n",
    "            # Take a move\n",
    "            r, s_prime = self.move(a)\n",
    "            total_reward += r\n",
    "            moves.append(a)\n",
    "\n",
    "            # Update the current state and ation\n",
    "            s = s_prime\n",
    "            states.append(s.label)\n",
    "            \n",
    "            idx = idx + 1\n",
    "        \n",
    "        return states, moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#demo_w = World(7, 7, special_cells=[((0, 3), 'start'), ((6, 3), 'goal'),\n",
    "#                               ((2, 2), 'trap3'), ((2, 4), 'trap3'), \n",
    "#                               ((3, 2), 'trap3'), ((3, 3), 'trap1'), ((3, 4), 'trap3'), \n",
    "#                               ((4, 2), 'trap3'), ((4, 4), 'trap3')])\n",
    "demo_w = World(7, 7, special_cells=[((0, 2), 'start'), ((6, 4), 'goal'),\n",
    "                               ((2, 2), 'trap3'), ((2, 4), 'trap3'), \n",
    "                               ((3, 2), 'trap3'), ((3, 4), 'trap3'), \n",
    "                               ((4, 2), 'trap3'), ((4, 4), 'trap3')])\n",
    "plt.figure(figsize=(5, 5))\n",
    "demo_w.draw_map()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an agent and perform a few episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random.seed(random_seed) \n",
    "a = RL_agent(demo_w, demo_w.start_cell, kind = \"Q_Learning\", epsilon = epsilon)\n",
    "display(a.pandas_action_value_function_table)\n",
    "ql_eps = a.perform_RL_episodes(2, verbose = 1, heatmap_layout = 'line', image_filename_root = 'rl_grid_world_q_learning')\n",
    "df = pd.DataFrame(ql_eps)\n",
    "ax = df.loc[0:100, 'return'].plot(color = 'black')\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run another episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_seed) \n",
    "a = RL_agent(demo_w, demo_w.start_cell, kind = \"Q_Learning\", epsilon = epsilon)\n",
    "ql_eps = a.perform_RL_episodes(1, verbose = 1, heatmap_layout = 'line')\n",
    "a.print_action_value_function_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the agent for 350 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(random_seed) \n",
    "a = RL_agent(demo_w, demo_w.start_cell, kind = \"Q_Learning\", epsilon = epsilon)\n",
    "ql_eps = a.perform_RL_episodes(num_episodes, heatmap_layout = 'line',image_filename_root = \"rl_grid_world_q_learning_\", verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evoloution  of the reward thriough the learning episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ql_eps)\n",
    "ax = df.loc[0:num_episodes, 'return'].plot(color = 'lightgray', xlim = [-5, num_episodes])\n",
    "df['return'].rolling(10).mean().plot(color = 'black', xlim = [-5, num_episodes])\n",
    "ax.set_xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rolling Mean (10) Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the agent to navigate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = a.perform_offline_task()\n",
    "print(s)\n",
    "print(m)\n",
    "demo_w.draw_map_visited_states_labels(s, m, \"rl_grid_world_q_learning__offline_path_arrows.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an agent an perform some episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(random_seed) \n",
    "a = RL_agent(demo_w, demo_w.start_cell, kind = \"SARSA\", epsilon = epsilon)\n",
    "sarsa_eps = a.perform_RL_episodes(num_episodes, verbose = 1, heatmap_layout = 'line', image_filename_root = \"rl_grid_world_sarsa_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sarsa_eps)\n",
    "ax = df.loc[0:num_episodes, 'return'].plot(color = 'lightgray', xlim = [-5, num_episodes])\n",
    "df['return'].rolling(10).mean().plot(color = 'black', xlim = [-5, num_episodes])\n",
    "ax.set_xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rolling Mean (10) Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = a.perform_offline_task()\n",
    "print(s)\n",
    "print(m)\n",
    "demo_w.draw_map_visited_states_labels(s, m, \"rl_grid_world_sarsa__offline_path_arrows.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
